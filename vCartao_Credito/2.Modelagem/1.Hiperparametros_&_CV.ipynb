{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos\n",
    "\n",
    "Primeiro, algumas variáveis serão descartadas pelo acúmulo de importância (~90%) e pelo valor de IV.\n",
    "\n",
    "Segundo, com o uso da validação cruzada: será avaliado o comportamento médio de algumas métricas calculadas nos modelos baselines como a curva sobre a curva precision e recall (AUCPR), Brier Score (BS) e log-loss; foi visto que a identificação de um melhor ponto de corte pode melhorar a métrica F1-score. Dessa forma, será calculado qual o melhor ponto de corte médio.\n",
    "\n",
    "Terceiro, será escolhido qual o melhor conjunto de hiperparâmetros para os modelos Random Forest e XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from deltalake import DeltaTable, write_deltalake\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import Funcoes\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score, average_precision_score, brier_score_loss, confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, log_loss\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, Trials, hp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura da base v1 e filtro de variáveis\n",
    "\n",
    "Variáveis com valor acumulado de importância em ~90% e pelo valor de IV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Age</th>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <th>Contacts_Count_12_mon</th>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "      <th>vfm</th>\n",
       "      <th>pmcc</th>\n",
       "      <th>Income_Category_1.&lt; 40k</th>\n",
       "      <th>Income_Category_2. &gt;= 40k &amp; &lt; 60k</th>\n",
       "      <th>Income_Category_3. &gt;= 60k &amp; &lt; 80k</th>\n",
       "      <th>Income_Category_4. &gt;= 80k &amp; &lt; 120k</th>\n",
       "      <th>Income_Category_5. &gt;= 120k</th>\n",
       "      <th>Education_Level_v2_1.Uneducated</th>\n",
       "      <th>Education_Level_v2_2.High School</th>\n",
       "      <th>Education_Level_v2_3.Graduate</th>\n",
       "      <th>Education_Level_v2_4.Post-Graduate</th>\n",
       "      <th>Attrition_Flag</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.784196</td>\n",
       "      <td>1.403132</td>\n",
       "      <td>-1.337898</td>\n",
       "      <td>0.498943</td>\n",
       "      <td>0.963894</td>\n",
       "      <td>0.282975</td>\n",
       "      <td>-0.328225</td>\n",
       "      <td>-0.175537</td>\n",
       "      <td>-0.421450</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Treino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720070</td>\n",
       "      <td>-0.525933</td>\n",
       "      <td>0.641818</td>\n",
       "      <td>1.408428</td>\n",
       "      <td>-0.165769</td>\n",
       "      <td>-1.527806</td>\n",
       "      <td>-0.194304</td>\n",
       "      <td>-0.208685</td>\n",
       "      <td>-1.054789</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Treino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.346848</td>\n",
       "      <td>-0.525933</td>\n",
       "      <td>-0.348040</td>\n",
       "      <td>0.498943</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.894171</td>\n",
       "      <td>0.056797</td>\n",
       "      <td>-0.571459</td>\n",
       "      <td>-0.686436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Treino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.218648</td>\n",
       "      <td>-0.525933</td>\n",
       "      <td>0.641818</td>\n",
       "      <td>-1.320028</td>\n",
       "      <td>-0.412731</td>\n",
       "      <td>0.369637</td>\n",
       "      <td>0.851953</td>\n",
       "      <td>0.252749</td>\n",
       "      <td>2.406712</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Treino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.539173</td>\n",
       "      <td>0.117089</td>\n",
       "      <td>1.631675</td>\n",
       "      <td>1.408428</td>\n",
       "      <td>-0.858972</td>\n",
       "      <td>0.346832</td>\n",
       "      <td>-1.144306</td>\n",
       "      <td>-0.064053</td>\n",
       "      <td>-0.071911</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Treino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Customer_Age  Total_Relationship_Count  Months_Inactive_12_mon  \\\n",
       "0     -0.784196                  1.403132               -1.337898   \n",
       "1      0.720070                 -0.525933                0.641818   \n",
       "2      1.346848                 -0.525933               -0.348040   \n",
       "3      0.218648                 -0.525933                0.641818   \n",
       "4     -2.539173                  0.117089                1.631675   \n",
       "\n",
       "   Contacts_Count_12_mon  Total_Revolving_Bal  Total_Amt_Chng_Q4_Q1  \\\n",
       "0               0.498943             0.963894              0.282975   \n",
       "1               1.408428            -0.165769             -1.527806   \n",
       "2               0.498943             0.864865              0.894171   \n",
       "3              -1.320028            -0.412731              0.369637   \n",
       "4               1.408428            -0.858972              0.346832   \n",
       "\n",
       "   Total_Ct_Chng_Q4_Q1       vfm      pmcc  Income_Category_1.< 40k  \\\n",
       "0            -0.328225 -0.175537 -0.421450                        1   \n",
       "1            -0.194304 -0.208685 -1.054789                        0   \n",
       "2             0.056797 -0.571459 -0.686436                        0   \n",
       "3             0.851953  0.252749  2.406712                        1   \n",
       "4            -1.144306 -0.064053 -0.071911                        1   \n",
       "\n",
       "   Income_Category_2. >= 40k & < 60k  Income_Category_3. >= 60k & < 80k  \\\n",
       "0                                  0                                  0   \n",
       "1                                  0                                  0   \n",
       "2                                  0                                  0   \n",
       "3                                  0                                  0   \n",
       "4                                  0                                  0   \n",
       "\n",
       "   Income_Category_4. >= 80k & < 120k  Income_Category_5. >= 120k  \\\n",
       "0                                   0                           0   \n",
       "1                                   1                           0   \n",
       "2                                   1                           0   \n",
       "3                                   0                           0   \n",
       "4                                   0                           0   \n",
       "\n",
       "   Education_Level_v2_1.Uneducated  Education_Level_v2_2.High School  \\\n",
       "0                                0                                 0   \n",
       "1                                0                                 0   \n",
       "2                                0                                 0   \n",
       "3                                0                                 0   \n",
       "4                                1                                 0   \n",
       "\n",
       "   Education_Level_v2_3.Graduate  Education_Level_v2_4.Post-Graduate  \\\n",
       "0                              1                                   0   \n",
       "1                              1                                   0   \n",
       "2                              0                                   1   \n",
       "3                              1                                   0   \n",
       "4                              0                                   0   \n",
       "\n",
       "   Attrition_Flag    type  \n",
       "0               0  Treino  \n",
       "1               0  Treino  \n",
       "2               0  Treino  \n",
       "3               0  Treino  \n",
       "4               0  Treino  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A chance de ser ou não churn não depende do sexo ser M ou F. Mas, ela pode ser uma proxy para comportamentos socioeconômicos como diferença de renda, ocupação em \n",
    "# atividades não remuneradas (múltiplas jornadas de trabalho) etc\n",
    "\n",
    "# Estado civil e número de dependentes podem ser interpretados de forma semelhante. Ambas podem direcionar para um maior compromisso com a estabilidade financeira,\n",
    "# pessoas com mais idade e, possivelmente, com principalidade de uso do cartão já definida.\n",
    "\n",
    "dados = DeltaTable(\"../1.Variaveis/tmp/dados_pp_v1\").to_pandas()\n",
    "dados.drop(['__index_level_0__', 'Card_Category_Gold', 'Card_Category_Platinum', 'Card_Category_Silver', \n",
    "            'Marital_Status_Married', 'Marital_Status_Single', 'Marital_Status_Unknown', 'Gender_M',\n",
    "            'Dependent_count'], axis=1, inplace=True)\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10127, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separação das bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_treino = dados[dados.type == 'Treino'].drop(['type'], axis=1)\n",
    "dados_val = dados[dados.type == 'Validacao'].drop(['type'], axis=1)\n",
    "dados_teste = dados[dados.type == 'Teste'].drop(['type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino = dados_treino.drop(['Attrition_Flag'], axis=1)\n",
    "y_treino = dados_treino['Attrition_Flag']\n",
    "\n",
    "X_val = dados_val.drop(['Attrition_Flag'], axis=1)\n",
    "y_val = dados_val['Attrition_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Attrition_Flag\n",
       " 0    6140\n",
       " 1    1175\n",
       " Name: count, dtype: int64,\n",
       " Attrition_Flag\n",
       " 0    1084\n",
       " 1     208\n",
       " Name: count, dtype: int64,\n",
       " Attrition_Flag\n",
       " 0    1276\n",
       " 1     244\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_treino.value_counts(), y_val.value_counts(), dados_teste['Attrition_Flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Será usado 5 folds de validação, para que a quantidade absoluta de churn seja equiparada a base de validação e teste (ver acima)\n",
    "1175/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validação cruzada\n",
    "\n",
    "Para ter uma avaliação mais precisa dos modelos baselines, será usada a validação cruzada. É importante ter em mente que precisa-se de uma quantidade razoável da variável target para o estudo, além de se assemelhar com a proporção verdadeira na base completa (~16%).\n",
    "\n",
    "Para calcular os escores, das diferentes métricas, foi usado o cross_val_score (https://scikit-learn.org/1.5/modules/cross_validation.html).\n",
    "\n",
    "Para a escolha das métricas de estudo, pode-se escolher pela lista definida em 3.4.3.1 (https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construção dos folds de forma estratificada para garantir a mesma representatividade da target\n",
    "\n",
    "SKF = StratifiedKFold(n_splits=5, shuffle=True, random_state=563)\n",
    "\n",
    "#  Instancia a Random Forest\n",
    "\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "\n",
    "# Aplicação da validação cruzada e obtenção dos escores\n",
    "\n",
    "auc_pr_rf = cross_val_score(estimator=rf, X=X_treino, y=y_treino, cv=SKF, scoring='average_precision')\n",
    "bs_rf = cross_val_score(estimator=rf, X=X_treino, y=y_treino, cv=SKF, scoring='neg_brier_score')\n",
    "log_loss_rf = cross_val_score(estimator=rf, X=X_treino, y=y_treino, cv=SKF, scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No mlflow, os valores de AUCPR e BS na base de validação foram, respectivamente, 0.80 e 0.06. Comparado com o resultado médio das métricas abaixo, pode-se concluir que o modelo com os hiperparâmetros default está com performance boa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor médio de AUCPR: 0.8110720675084535\n",
      "Desvio padrão de AUCPR: 0.018858549324494565\n",
      "----------------------------------------\n",
      "Valor médio do BS: 0.06355405331510594\n",
      "Desvio padrão do BS: 0.0023251419277794854\n",
      "----------------------------------------\n",
      "Valor médio da log-loss: 0.23914582093998044\n",
      "Desvio padrão da log-loss: 0.017892713962885703\n"
     ]
    }
   ],
   "source": [
    "print('Valor médio de AUCPR:', auc_pr_rf.mean())\n",
    "print('Desvio padrão de AUCPR:', auc_pr_rf.std())\n",
    "\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('Valor médio do BS:', bs_rf.mean()*(-1))\n",
    "print('Desvio padrão do BS:', bs_rf.std())\n",
    "\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('Valor médio da log-loss:', log_loss_rf.mean()*(-1))\n",
    "print('Desvio padrão da log-loss:', log_loss_rf.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ao usar o XGBoost, as colunas não podem ter sinais de '>', '>=', '<' e '<='\n",
    "X_treino_new = X_treino.rename(columns={'Income_Category_1.< 40k': 'Income_Category_1.40k',\n",
    "                                        'Income_Category_2. >= 40k & < 60k': 'Income_Category_2.40k_60k',\n",
    "                                        'Income_Category_3. >= 60k & < 80k': 'Income_Category_3.60k_80k',\n",
    "                                        'Income_Category_4. >= 80k & < 120k': 'Income_Category_4.80k_120k',\n",
    "                                        'Income_Category_5. >= 120k': 'Income_Category_5.120k'\n",
    "                                        })\n",
    "\n",
    "X_val_new = X_val.rename(columns={'Income_Category_1.< 40k': 'Income_Category_1.40k',\n",
    "                                        'Income_Category_2. >= 40k & < 60k': 'Income_Category_2.40k_60k',\n",
    "                                        'Income_Category_3. >= 60k & < 80k': 'Income_Category_3.60k_80k',\n",
    "                                        'Income_Category_4. >= 80k & < 120k': 'Income_Category_4.80k_120k',\n",
    "                                        'Income_Category_5. >= 120k': 'Income_Category_5.120k'\n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construção dos folds de forma estratificada para garantir a mesma representatividade da target\n",
    "\n",
    "SKF = StratifiedKFold(n_splits=5, shuffle=True, random_state=563)\n",
    "\n",
    "#  Instancia a XGBoost\n",
    "\n",
    "XGB = xgb.XGBClassifier(random_state=123)\n",
    "\n",
    "# Aplicação da validação cruzada e obtenção dos escores\n",
    "\n",
    "auc_pr_XGB = cross_val_score(estimator=XGB, X=X_treino_new, y=y_treino, cv=SKF, scoring='average_precision')\n",
    "bs_XGB = cross_val_score(estimator=XGB, X=X_treino_new, y=y_treino, cv=SKF, scoring='neg_brier_score')\n",
    "log_loss_XGB = cross_val_score(estimator=XGB, X=X_treino_new, y=y_treino, cv=SKF, scoring='neg_log_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No mlflow, os valores de AUCPR e BS na base de validação foram, respectivamente, 0.85 e 0.05. Comparado com o resultado médio das métricas abaixo, pode-se concluir que o modelo com os hiperparâmetros default está com performance boa.\n",
    "\n",
    "Em média os valores das métricas são melhores para o XGB, entretanto perdem um pouco em relação à variância que é um pouco maior (trade-off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor médio de AUCPR: 0.827286855808228\n",
      "Desvio padrão de AUCPR: 0.01765843965399739\n",
      "----------------------------------------\n",
      "Valor médio do BS: 0.059690328309613225\n",
      "Desvio padrão do BS: 0.004629364361334095\n",
      "----------------------------------------\n",
      "Valor médio da log-loss: 0.21693788939008582\n",
      "Desvio padrão da log-loss: 0.01650408191605959\n"
     ]
    }
   ],
   "source": [
    "print('Valor médio de AUCPR:', auc_pr_XGB.mean())\n",
    "print('Desvio padrão de AUCPR:', auc_pr_XGB.std())\n",
    "\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('Valor médio do BS:', bs_XGB.mean()*(-1))\n",
    "print('Desvio padrão do BS:', bs_XGB.std())\n",
    "\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('Valor médio da log-loss:', log_loss_XGB.mean()*(-1))\n",
    "print('Desvio padrão da log-loss:', log_loss_XGB.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do melhor ponto de corte médio com Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "pontos_rf = []\n",
    "f1_rf = []\n",
    "\n",
    "for i, (treino_index, teste_index) in enumerate(SKF.split(X_treino, y_treino)):\n",
    "    \n",
    "    # Marcação do fold\n",
    "    \n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    # Divisão das bases de treino e teste\n",
    "    \n",
    "    X_treino_aux, X_teste_aux = X_treino.iloc[treino_index,:], X_treino.iloc[teste_index, :]\n",
    "    y_treino_aux, y_teste_aux = y_treino[treino_index], y_treino[teste_index]\n",
    "\n",
    "    # Aplicação da RF\n",
    "    \n",
    "    rf.fit(X_treino_aux, y_treino_aux)\n",
    "\n",
    "    # Avaliação do melhor ponto de corte com o uso da base de validação\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_teste_aux, rf.predict_proba(X_teste_aux)[:,1])\n",
    "\n",
    "    # Melhor threshold que fornece o melhor f1-score (média harmônica entre precisão (Positive Predicted Value ou PPV) e recall (TPR))\n",
    "\n",
    "    f1_scores = 2 * (precision*recall) / (precision + recall)\n",
    "    best_threshold_index = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_threshold_index]\n",
    "    best_f1 = f1_scores[best_threshold_index]\n",
    "    \n",
    "    pontos_rf.append(best_threshold)\n",
    "    f1_rf.append(best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em média, o melhor ponto de corte é 0.356, garantindo em média um f1-score de 0.747. No mlflow, observa-se que a métrica f1-score na base de validação ficou em aproximadamente 0.686 (com o uso do ponto de corte default em 0.5), ou seja, é possível ter um ganho com a otimização do ponto de corte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.float64(0.37),\n",
       "  np.float64(0.34),\n",
       "  np.float64(0.41),\n",
       "  np.float64(0.33),\n",
       "  np.float64(0.33)],\n",
       " [np.float64(0.726086956521739),\n",
       "  np.float64(0.7695560253699788),\n",
       "  np.float64(0.7527839643652562),\n",
       "  np.float64(0.7418032786885246),\n",
       "  np.float64(0.7479674796747968)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pontos_rf, f1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.356),\n",
       " np.float64(0.03072458299147442),\n",
       " np.float64(0.747639540924059))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pontos_rf), np.std(pontos_rf), np.mean(f1_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do melhor ponto de corte médio com XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "pontos_XGB = []\n",
    "f1_XGB = []\n",
    "\n",
    "for i, (treino_index, teste_index) in enumerate(SKF.split(X_treino_new, y_treino)):\n",
    "    \n",
    "    # Marcação do fold\n",
    "    \n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    # Divisão das bases de treino e teste\n",
    "    \n",
    "    X_treino_aux, X_teste_aux = X_treino_new.iloc[treino_index,:], X_treino_new.iloc[teste_index, :]\n",
    "    y_treino_aux, y_teste_aux = y_treino[treino_index], y_treino[teste_index]\n",
    "\n",
    "    # Aplicação da RF\n",
    "    \n",
    "    XGB.fit(X_treino_aux, y_treino_aux)\n",
    "\n",
    "    # Avaliação do melhor ponto de corte com o uso da base de validação\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_teste_aux, XGB.predict_proba(X_teste_aux)[:,1])\n",
    "\n",
    "    # Melhor threshold que fornece o melhor f1-score (média harmônica entre precisão (Positive Predicted Value ou PPV) e recall (TPR))\n",
    "\n",
    "    f1_scores = 2 * (precision*recall) / (precision + recall)\n",
    "    best_threshold_index = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_threshold_index]\n",
    "    best_f1 = f1_scores[best_threshold_index]\n",
    "    \n",
    "    pontos_XGB.append(best_threshold)\n",
    "    f1_XGB.append(best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em média, o melhor ponto de corte é 0.369, garantindo em média um f1-score de 0.755. No mlflow, observa-se que a métrica f1-score na base de validação ficou em aproximadamente 0.755 (com o uso do ponto de corte default em 0.5), ou seja, não houve alteração nessa métrica.\n",
    "\n",
    "Note que a variabilidade (desvio padrão) dos pontos de corte é maior comparado com o modelo Random Forest. Além disso, o ponto de corte médio para a RF é bem próximo do ponto de corte médio do XGBoost e os dois modelos possuem f1-score médio próximos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.float32(0.43493217),\n",
       "  np.float32(0.30468673),\n",
       "  np.float32(0.25663266),\n",
       "  np.float32(0.3706298),\n",
       "  np.float32(0.48277158)],\n",
       " [np.float64(0.7175925925925927),\n",
       "  np.float64(0.7748917748917749),\n",
       "  np.float64(0.7505154639175258),\n",
       "  np.float64(0.7583148558758315),\n",
       "  np.float64(0.7770114942528736)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pontos_XGB, f1_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float32(0.3699306),\n",
       " np.float32(0.082525104),\n",
       " np.float64(0.7556652363061197))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pontos_XGB), np.std(pontos_XGB), np.mean(f1_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão 1\n",
    "\n",
    "Dos resultados anteriores, percebe-se que a diferença é pequena (e praticamente desprezível) entre as métricas médias dos modelos ajustados. Dessa forma, pode-se optar por um modelo mais simples como é o caso da RF.\n",
    "\n",
    "Como forma de recapitular conceitos chaves, o modelo RF faz parte dos métodos de bagging. Neste grupo, as principais características são: possibilidade de execução dos algoritmos de forma paralela e/ou distribuída; redução de variância e menor sensibilidade a overfitting.\n",
    "\n",
    "O modelo XGBoost faz parte dos métodos de boosting, com as seguintes características: a execução dos algoritmos é feita de forma sequencial, com enfoque em melhorar as predições nos grupos em que há mais erros e assim não pode ser executado de forma paralela e/ou distribuída; redução de viés e sensibilidade a overfitting (uma vez que o foco está nas observações com as piores predições)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Attrition_Flag\n",
       " 0    3684\n",
       " 1     705\n",
       " Name: count, dtype: int64,\n",
       " Attrition_Flag\n",
       " 0    2456\n",
       " 1     470\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treino2, X_teste2, y_treino2, y_teste2 = train_test_split(X_treino_new, y_treino, test_size=.40, stratify=y_treino, random_state=1234)\n",
    "y_treino2.value_counts(), y_teste2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode ser que em alguma etapa de seleção de hiperparâmetros, algumas métricas não sejam bem definidas como precisão e recall, pois as medidas de falso positivo e verdadeiro positivo são iguais a zero ou falso negativo e verdadeiro positivo são iguais a zero. Assim, essas métricas foram desconsideras na função objetivo e mapeadas somente no final, na base de validação final.\n",
    "\n",
    "Abaixo, a função objetivo tem como alvo minimizar a função log-loss e utiliza-se a otimização bayesiana para alcançar o melhor conjunto de hiperparâmetros (https://hyperopt.github.io/hyperopt/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função objetivo sem a validação cruzada\n",
    "\n",
    "def func_objetivo(parametros, expr, modelo, X_treino, X_teste, y_treino, y_teste):\n",
    "    # função objetivo para \"minimizar\", mas dependendo da métrica de interesse, na realidade, é maximizar \n",
    "    # parâmetros é o espaço paramétrico a ser explorado\n",
    "    # expr é uma string que representa o id do experimento que foi criado\n",
    "    # modelo é uma string de qual modelo será rodado: Random Forest ou XGBoost\n",
    "    # X_treino, X_teste, y_treino, y_teste são as bases de treino e teste \n",
    "\n",
    "    # O output é o valor do score a ser minimizado/maximizado\n",
    "    \n",
    "    with mlflow.start_run(nested = True, experiment_id=expr) as run:\n",
    "\n",
    "        if modelo == 'RF':\n",
    "            clf = RandomForestClassifier(**parametros) \n",
    "            clf.fit(X_treino, y_treino)\n",
    "        elif modelo == 'XGB':\n",
    "            clf = xgb.XGBClassifier(**parametros)\n",
    "            clf.fit(X_treino, y_treino)\n",
    "        \n",
    "        #score = log_loss(y_teste, clf.predict_proba(X_teste)[:,1], normalize=False)\n",
    "        score = log_loss(y_teste, clf.predict_proba(X_teste)[:,1], normalize=True)\n",
    "\n",
    "        # Log de parâmetros e métricas \n",
    "\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('log_loss_val', score)\n",
    "        mlflow.log_metric('aucpr_val', average_precision_score(y_teste, clf.predict_proba(X_teste)[:,1]))\n",
    "        mlflow.log_metric('roc_auc_score_val', roc_auc_score(y_teste, clf.predict_proba(X_teste)[:,1]))\n",
    "        #mlflow.log_metric('f1_score_val', f1_score(y_teste, clf.predict(X_teste)))\n",
    "        #mlflow.log_metric('precision_score_val', precision_score(y_teste, clf.predict(X_teste)))\n",
    "        #mlflow.log_metric('recall_score_val', recall_score(y_teste, clf.predict(X_teste)))\n",
    "        \n",
    "        signature = infer_signature(X_treino, clf.predict_proba(X_treino))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# Função objetivo com validação cruzada\n",
    "# A otimização será feita considerando o comportamento médio da log-loss nos folds de validação\n",
    "\n",
    "def func_objetivo_CV(parametros, modelo, folds, expr, X, y):\n",
    "    # função objetivo para \"minimizar\", mas dependendo da métrica de interesse, na realidade, é maximizar \n",
    "    # parametros é o espaço paramétrico a ser explorado\n",
    "    # expr é uma string que representa o id do experimento que foi criado\n",
    "    # modelo é uma string de qual modelo será rodado: Random Forest ou XGBoost\n",
    "    # folds é um int que diz quantos folds de validação serão usados\n",
    "    # X e y são as bases que serão aplicadas o cross-validation\n",
    "\n",
    "    # O output é o valor do score a ser minimizado/maximizado\n",
    "    \n",
    "    with mlflow.start_run(nested = True, experiment_id=expr) as run:\n",
    "\n",
    "        SKF = StratifiedKFold(n_splits = folds, shuffle=True, random_state=1234)\n",
    "\n",
    "        if modelo == 'RF':\n",
    "            clf = RandomForestClassifier(**parametros) \n",
    "            clf.fit(X, y)\n",
    "        elif modelo == 'XGB':\n",
    "            clf = xgb.XGBClassifier(**parametros)\n",
    "            clf.fit(X, y)\n",
    "        \n",
    "        #score = cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='average_precision').mean()\n",
    "        score = cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='neg_log_loss').mean()\n",
    "\n",
    "        # Log de parâmetros e métricas\n",
    "\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('average_precision_cv', cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='average_precision').mean())\n",
    "        mlflow.log_metric('roc_auc_cv', cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='roc_auc').mean())\n",
    "        mlflow.log_metric('neg_brier_score_cv', cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='neg_brier_score').mean())\n",
    "        mlflow.log_metric('neg_log_loss_cv', cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='neg_log_loss').mean())\n",
    "        \n",
    "        signature = infer_signature(X, clf.predict_proba(X))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')\n",
    "\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment = mlflow.create_experiment(name = 'Modelos_Tunados',\n",
    "#                                      artifact_location = 'Artf_Modelos_Tunados',\n",
    "#                                      tags = {'Environment': 'Development', 'Version': '2.0.0'}\n",
    "#                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'840998848624696013'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.set_experiment(experiment_id='840998848624696013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'840998848624696013'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparâmetros sem validação cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espaço hiperparamétrico com 6 dimensões: quantidade de árvores, profundidade máxima, quantidade mínima de observações na folha, quantidade mínima de amostras para divisão do nó, critério para a divisão do nó e o peso atribuído as amostras devido ao desbalanceamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:04<01:37,  4.07s/trial, best loss: 0.5243437337037734]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:08<01:34,  4.12s/trial, best loss: 0.3383480247955717]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:13<01:36,  4.37s/trial, best loss: 0.3383480247955717]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #test_imbalanced = [{0: len(y_treino2)/(2*np.bincount(y_treino2))[0], 1:len(y_teste2)/(2*np.bincount(y_teste2))[1]}, {0: 1, 1:1}]\n",
    "    \n",
    "    space = {\n",
    "        \"n_estimators\": hp.choice('n_estimators', np.arange(10, 500, dtype=int)),\n",
    "        \"max_depth\": hp.choice('max_depth', np.arange(10, 300, dtype=int)),\n",
    "        \"min_samples_leaf\": hp.choice('min_samples_leaf', np.arange(200, 500, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice('min_samples_split', np.arange(200, 500, dtype=int)),\n",
    "        \"criterion\": hp.choice(\"criterion\", ['gini', 'entropy', 'log_loss']),\n",
    "        \"class_weight\": hp.choice(\"class_weight\", ['balanced', 'balanced_subsample', None]) \n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(run_name = 'Random_Forest1', experiment_id=experiment.experiment_id) as run:\n",
    "        best_params = fmin(\n",
    "            fn = partial(\n",
    "                func_objetivo,\n",
    "                expr = experiment.experiment_id,\n",
    "                modelo = 'RF',\n",
    "                X_treino = X_treino2,\n",
    "                X_teste = X_teste2,\n",
    "                y_treino = y_treino2,\n",
    "                y_teste = y_teste2\n",
    "            ),\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 25,\n",
    "            trials = Trials(),\n",
    "            timeout = 10\n",
    "        )\n",
    "\n",
    "        if best_params['criterion'] == 0:\n",
    "            best_params['criterion'] = 'gini'\n",
    "        elif best_params['criterion'] == 1:\n",
    "            best_params['criterion'] = 'entropy'\n",
    "        else:\n",
    "            best_params['criterion'] = 'log_loss'\n",
    "            \n",
    "\n",
    "        if best_params['class_weight'] == 0:\n",
    "            best_params['class_weight'] = 'balanced'\n",
    "        elif best_params['class_weight'] == 1:\n",
    "            best_params['class_weight'] = 'balanced_subsample'\n",
    "        else:\n",
    "            best_params['class_weight'] = None\n",
    "        \n",
    "        # Identificado o melhor conjunto de hiperparâmetros, treina o modelo com toda a base de treino e metrifica os escores na base de validação\n",
    "\n",
    "        clf = RandomForestClassifier(**best_params)\n",
    "        clf.fit(X_treino_new, y_treino)\n",
    "                   \n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('aucpr_val', average_precision_score(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "        mlflow.log_metric('log_loss_val', log_loss(y_val, clf.predict_proba(X_val_new)[:,1], normalize=True))\n",
    "        mlflow.log_metric('roc_auc_score_val', roc_auc_score(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "        mlflow.log_metric('f1_score_val', f1_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('f1_score_val2', f1_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('precision_score_val', precision_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('precision_score_val2', precision_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('recall_score_val', recall_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('recall_score_val2', recall_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('brier_score', brier_score_loss(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "\n",
    "        signature = infer_signature(X_treino_new, clf.predict_proba(X_treino_new))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espaço hiperparamétrico com 2 dimensões, para alterar o menos possível os hiperparâmetros default, com os quais já se havia encontrado bons resultados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:03<01:33,  3.91s/trial, best loss: 0.2952681827606543]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:11<02:14,  5.87s/trial, best loss: 0.2254783429168717]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    #test_imbalanced = [{0: len(y_treino2)/(2*np.bincount(y_treino2))[0], 1:len(y_teste2)/(2*np.bincount(y_teste2))[1]}, {0: 1, 1:1}]\n",
    "    \n",
    "    space = {\n",
    "        \"n_estimators\": hp.choice('n_estimators', np.arange(10, 500, dtype=int)),\n",
    "        \"max_depth\": hp.choice('max_depth', np.arange(10, 300, dtype=int))\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(run_name = 'Random_Forest2', experiment_id=experiment.experiment_id) as run:\n",
    "        best_params = fmin(\n",
    "            fn = partial(\n",
    "                func_objetivo,\n",
    "                expr = experiment.experiment_id,\n",
    "                modelo = 'RF',\n",
    "                X_treino = X_treino2,\n",
    "                X_teste = X_teste2,\n",
    "                y_treino = y_treino2,\n",
    "                y_teste = y_teste2\n",
    "            ),\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 25,\n",
    "            trials = Trials(),\n",
    "            timeout = 10\n",
    "        )\n",
    "       \n",
    "        # Identificado o melhor conjunto de hiperparâmetros, treina o modelo com toda a base de treino e metrifica os escores na base de validação\n",
    "\n",
    "        clf = RandomForestClassifier(**best_params)\n",
    "        clf.fit(X_treino_new, y_treino)\n",
    "                   \n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('aucpr_val', average_precision_score(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "        mlflow.log_metric('log_loss_val', log_loss(y_val, clf.predict_proba(X_val_new)[:,1], normalize=True))\n",
    "        mlflow.log_metric('roc_auc_score_val', roc_auc_score(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "        mlflow.log_metric('f1_score_val', f1_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('f1_score_val2', f1_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('precision_score_val', precision_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('precision_score_val2', precision_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('recall_score_val', recall_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('recall_score_val2', recall_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('brier_score', brier_score_loss(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "\n",
    "\n",
    "        signature = infer_signature(X_treino_new, clf.predict_proba(X_treino_new))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparâmetros com validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:10<04:10, 10.45s/trial, best loss: 0.49348483924992664]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    space = {\n",
    "        \"n_estimators\": hp.choice('n_estimators', np.arange(10, 500, dtype=int)),\n",
    "        \"max_depth\": hp.choice('max_depth', np.arange(10, 300, dtype=int)),\n",
    "        \"min_samples_leaf\": hp.choice('min_samples_leaf', np.arange(200, 500, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice('min_samples_split', np.arange(200, 500, dtype=int)),\n",
    "        \"criterion\": hp.choice(\"criterion\", ['gini', 'entropy', 'log_loss']),\n",
    "        \"class_weight\": hp.choice(\"class_weight\", ['balanced', 'balanced_subsample', None]) \n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(run_name = 'Random_Forest_CV1', experiment_id=experiment.experiment_id) as run:\n",
    "        best_params = fmin(\n",
    "            fn = partial(\n",
    "                func_objetivo_CV,\n",
    "                expr = experiment.experiment_id,\n",
    "                modelo = 'RF',\n",
    "                X = X_treino_new,\n",
    "                y = y_treino,\n",
    "                folds = 5\n",
    "            ),\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 25,\n",
    "            trials = Trials(),\n",
    "            timeout = 10\n",
    "        )\n",
    "\n",
    "        if best_params['criterion'] == 0:\n",
    "            best_params['criterion'] = 'gini'\n",
    "        elif best_params['criterion'] == 1:\n",
    "            best_params['criterion'] = 'entropy'\n",
    "        else:\n",
    "            best_params['criterion'] = 'log_loss'\n",
    "            \n",
    "\n",
    "        if best_params['class_weight'] == 0:\n",
    "            best_params['class_weight'] = 'balanced'\n",
    "        elif best_params['class_weight'] == 1:\n",
    "            best_params['class_weight'] = 'balanced_subsample'\n",
    "        else:\n",
    "            best_params['class_weight'] = None\n",
    "        \n",
    "        # Identificado o melhor conjunto de hiperparâmetros, treina o modelo com toda a base de treino e metrifica os escores na base de validação\n",
    "\n",
    "        clf = RandomForestClassifier(**best_params)\n",
    "        clf.fit(X_treino_new, y_treino)\n",
    "                   \n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('aucpr_val', average_precision_score(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "        mlflow.log_metric('log_loss_val', log_loss(y_val, clf.predict_proba(X_val_new)[:,1], normalize=True))\n",
    "        mlflow.log_metric('roc_auc_score_val', roc_auc_score(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "        mlflow.log_metric('f1_score_val', f1_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('f1_score_val2', f1_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('precision_score_val', precision_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('precision_score_val2', precision_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('recall_score_val', recall_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('recall_score_val2', recall_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('brier_score', brier_score_loss(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "\n",
    "        signature = infer_signature(X_treino_new, clf.predict_proba(X_treino_new))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:17<07:08, 17.86s/trial, best loss: 0.27413191438820855]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Cartao_Credito/vCartao_Credito/lib/python3.12/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    space = {\n",
    "        \"n_estimators\": hp.choice('n_estimators', np.arange(10, 500, dtype=int)),\n",
    "        \"max_depth\": hp.choice('max_depth', np.arange(10, 300, dtype=int))\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(run_name = 'Random_Forest_CV2', experiment_id=experiment.experiment_id) as run:\n",
    "        best_params = fmin(\n",
    "            fn = partial(\n",
    "                func_objetivo_CV,\n",
    "                expr = experiment.experiment_id,\n",
    "                modelo = 'RF',\n",
    "                X = X_treino_new,\n",
    "                y = y_treino,\n",
    "                folds = 5\n",
    "            ),\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 25,\n",
    "            trials = Trials(),\n",
    "            timeout = 10\n",
    "        )\n",
    "\n",
    "       \n",
    "        # Identificado o melhor conjunto de hiperparâmetros, treina o modelo com toda a base de treino e metrifica os escores na base de validação\n",
    "\n",
    "        clf = RandomForestClassifier(**best_params)\n",
    "        clf.fit(X_treino_new, y_treino)\n",
    "                   \n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('aucpr_val', average_precision_score(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "        mlflow.log_metric('log_loss_val', log_loss(y_val, clf.predict_proba(X_val_new)[:,1], normalize=True))\n",
    "        mlflow.log_metric('roc_auc_score_val', roc_auc_score(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "        mlflow.log_metric('f1_score_val', f1_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('f1_score_val2', f1_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('precision_score_val', precision_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('precision_score_val2', precision_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('recall_score_val', recall_score(y_val, clf.predict(X_val_new)))\n",
    "        mlflow.log_metric('recall_score_val2', recall_score(y_val, clf.predict_proba(X_val_new)[:,1] > 0.356))\n",
    "        mlflow.log_metric('brier_score', brier_score_loss(y_val, clf.predict_proba(X_val_new)[:,1]))\n",
    "\n",
    "        signature = infer_signature(X_treino_new, clf.predict_proba(X_treino_new))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vCartao_Credito",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
